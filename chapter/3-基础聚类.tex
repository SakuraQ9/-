\section{基础聚类}

\subsection{常见聚类方法}
\textbf{聚类}（clustering）是在“\textbf{无监督学习}”任务中研究最多、应用最广的一类方法。其目标是将数据样本划分为若干个通常不相交的“簇”(cluster)，既可以作为一个单独过程（用于找寻数据内在的分布结构），也可作为分类等其他学习任务的前驱过程。

各位宝子们可以欣赏故事一则：

老师拿来苹果和梨，让小朋友分成两份。

小明把大苹果大梨放一起，小个头的放一起，老师点头，嗯，体量感。

小芳把红苹果挑出来，剩下的放一起，老师点头，颜色感。

小武的结果？不明白。小武掏出眼镜：最新款，能看到水果里有几个籽，左边这堆单数，右边双数。

老师很高兴：新的聚类算法诞生了。

这则故事说明了什么呢？聚类的标准往往无需墨守陈规，而是基于我们当前的任务灵活选择的。因此，聚类也许是机器学习中“新算法”出现最多、最快的领域，总能找到一个新的“标准”，使以往算法对它无能为力。

常见的聚类方法可以分类如下：

  \textbf{原型聚类}

• 亦称“基于原型的聚类”(prototype-based clustering)

• 假设：聚类结构能通过一组原型刻画

• 过程：先对原型初始化，然后对原型进行迭代更新求解

• 代表：k均值聚类，学习向量量化(LVQ)，高斯混合聚类

  \textbf{密度聚类}

• 亦称“基于密度的聚类”(density-based clustering)

• 假设：聚类结构能通过样本分布的紧密程度确定

• 过程：从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇

• 代表：DBSCAN, OPTICS, DENCLUE

  \textbf{层次聚类} (hierarchical clustering)

• 假设：能够产生不同粒度的聚类结果

• 过程：在不同层次对数据集进行划分，从而形成树形的聚类结构

• 代表：AGNES (自底向上)，DIANA (自顶向下)
\newline % 强制换行

接下来，笔者将为大家介绍两种用的最多的聚类方式。
\subsection{K-Means聚类}
此法是原型聚类的一个古老且经典的算法，其原理很简单：首先随机地选取出若干个样本作为初始的均值向量（簇）；然后分别计算每一个样本点与初始均值向量的距离，与哪个均值向量最近就属于哪个簇；最后将每个簇重新计算中心点，重复第二步直到收敛。

在搞清操作步骤之后，我们需要了解在上述操作中都蕴含了什么知识呢？笔者概括如下：

（1）K-means 算法的核心原理是什么？

此算法最终肯定希望得到“簇内相似度”高且“簇间相似度”低的聚类效果，而对于 K-均值的理解，便是要找到一个最优解来刻画每一个簇内的样本围绕这个簇的均值（初始均值向量）的紧密程度高。用公式表达如下：
对于簇划分$C=\{c_{1},c_{2},...,c_{n}\}$，最小化平方误差：
\setcounter{equation}{0} %新的一章，将编号公式重设为1
\begin{equation}E=\sum_{i=1}^k\sum_{x\in c_i}\|x-\mu_i\|_2^2,\text{其中均值向量}\mu_i=\frac{1}{|c_i|}\sum_{x\in c_i}x\end{equation}

（2）K-means 算法为什么要做重复第二步的操作直到收敛？

因为要寻找最小化的 E 值是一个 NP 难问题，所以只能通过贪心的思想，通过一次又一次的迭代优化来近似求解。

（3）如何计算距离？

给定样本集$x_{i}=(x_{i1},x_{i2},...,x_{in})$与样本集$x_{j}=(x_{j1},x_{j2},...,x_{jn})$，通常使用\textbf{闵可夫斯基距离}（Minkowski distance）来计算：
\begin{equation}dist_{mk}(x_i,x_j)=(\sum_{\mu=1}^n\left|x_{iu}-x_{ju}\right|^p)^{1/p}\end{equation}
式（2）中，当$p\to\infty $时 ，则得到\textbf{切比雪夫距离}；

当 p=2 时，则得到\textbf{欧式距离}:
\begin{equation}dist_{ed}(x_i,x_j)=\sqrt{\sum_{\mu=1}^n\left|x_{iu}-x_{ju}\right|^2}\end{equation}

当 p=1 时，则得到\textbf{曼哈顿距离}：
\begin{equation}dist_{man}(x_i,x_j)=\sum_{\mu=1}^n\left|x_{iu}-x_{ju}\right|\end{equation}

在处理连续属性时通常使用上述的闵可夫斯基距离来计算，但当处理离散型属性时，闵可夫斯基距离就不再适用，此时将采用 VDM 距离进行计算：
\begin{equation}VDM_{p(a,b)}=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p\end{equation}
式（5）中，令$m_{u,a}$表示属性 u 上取值为 a的样本数，$m_{u,a,i}$表示在第 i 个样本簇中属性 u 上取值为 a 的样本数，k 为样本簇数，则表达两个离散数据 a 与 b之间的 VDM 距离。

若数据样本既有连续值又有离散值该怎么办呢？很简单，将闵可夫斯基距离与 VDM 距离混合起来，就可以处理混合数据：
\begin{equation}M\text{incov}DM_p=(\sum_{u=1}^{n_c}\left|x_{iu}-x_{ju}\right|^p+\sum_{u=n_c+1}^nVDM_p(x_{iu}-x_{ju}))^{1/p}\end{equation}

当样本空间中不同属性的重要性不同时，可以使用加权闵可夫斯基距离：
\begin{equation}dist_{wmk}\left(x_i,x_j\right)=\left(w_1\left|x_{i1}-x_{j1}\right|^p+...+w_n\left|x_{in}-x_{jn}\right|^p\right)^{1/p}\end{equation}

在现实任务中，有必要基于数据样本来确定合适的距离计算计算式。
\subsection{高斯混合聚类}
此法的基本原理是：高斯混合分布并不是高斯模型，而是若干个模型的混合物形态，哪个数据样本更加属于高斯模型，就被分到哪一类中。与 k-Means不同的是，高斯混合聚类是通过概率模型来表达聚类原型的方法。

基本操作步骤概括如下：
（1）初始化高斯混合成分的个数 k，假设高斯混合分布模型参数α为（高斯混合系数），u 为均值，$\Sigma $为协方差矩阵。

（2）分别计算每个样本点的后验概率（该样本点属于每一个高斯模型的概率）。

（3）迭代，重复第二步操作直至收敛。

通过分析不难发现，高斯混合聚类的难点就在于后验概率的计算（该样本点属于每一个高斯模型的概率）。具体做法参见方羽新《机器学习基础实验 GMM 和 PCA》，在此就不多做赘述了。
