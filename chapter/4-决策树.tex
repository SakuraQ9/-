\section{决策树}

\subsection{决策树的认识}
决策树是基于树的结构进行决策的算法，每个“内部结点”对应于某个属性上的“测试”，每个分支对应于该测试的一种可能结果（即该属性的某个取值），每个“叶结点”对应于一个“预测结果”。

我们可以用两种视角来理解决策树：

其一，可以将决策树看作是一条由\textbf{if-then}规则构成的链条，即从根结点到每一个叶子结点的每一条路径都构建一个规则，那么这条路径中的内部结点特征表示\textbf{规则的条件}，其对应的叶子结点则表示\textbf{规则的结论}。

其二，可以将决策树模型理解为一种概率模型。若X为特征的随机变量，Y为类的随机变量，相应的条件概率分布可以表示为：$P(Y \mid X)$。当叶子结点上的条件概率分布偏向于某一类时，那么属于该类的概率更大。

\subsection{决策树的基本流程}
处理决策树问题的核心策略是“\textbf{分而治之}”（divide-and-conquer），这种处理思路类似于数据结构讲排序算法中的快速排序。这样做是因为决策树的本质其实是自根至叶的递归过程，我们需要在每个中间结点寻找一个“划分”（split or test）属性。其基本流程可以概述如下：

\textbf{学习过程}：通过对训练样本的分析来确定“划分属性”（即内部结点所对应的属性）。

\textbf{预测过程}：将测试示例从根结点开始，沿着划分属性所构成的“判定测试序列”下行，直到叶结点。

下面引用周志华老师在西瓜书中所呈现的伪代码流程（如表2）：

上述的基本流程中，有以下几点值得关注：

（1）决策树基本流程中的三种\textbf{停止条件}有：

\textbf{情形一}：当前结点包含的样本全属于同一类别，无需划分（过程2~4）；

\textbf{情形二}：当前属性集为空，或是所有样本在所有属性上取值相同，无法划分（过程5~7）；

\textbf{情形三}：当前结点包含的样本集合为空，不能划分（过程11~13）。

（2）过程中蓝色矩形所标注的做法的实质是\textbf{利用当前结点的后验分布}；橙色矩形所标注的做法的实质是\textbf{将父结点的样本分布作为当前结点的先验分布}。

（3）过程中红色矩形所标注的做法是\textbf{决策树算法的核心}！从训练数据中众多的特征中选择一个最优特征作为当前结点的分裂标准的做法称为\textbf{特征选择}。

\begin{table}[t]
	\caption{决策树算法流程}
	\begin{center}
		\begin{tabular}{p{15cm}}
			\hline
			\textbf{输入}: 训练集 $D = \{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\}$; \\
			属性集 $A = \{\alpha_1, \alpha_2, ..., \alpha_d\}$; \\
			\hline
			\textbf{过程}: 函数 TreeGenerate($D$, $A$) \\
			1. 生成结点 node; \\
			2. \textbf{if}: $D$ 中样本全属于同一类别 $C$， \textbf{then} \\
			3. 将 node 标记为 $C$ 类叶结点; \textbf{return} \\
			4. \textbf{end if} \\
			5. \textbf{if}: $A = \emptyset$ or $D$ 中样本在 $A$ 上取值相同 \textbf{then} \\
			6. 将 node 标记为叶结点，\textbf{\textcolor{blue}{其类别标记为 $D$ 中样本数最多的类}}; \textbf{return} \\
			7. \textbf{end if} \\
			8. \textbf{\textcolor{red}{从 $A$ 中选择最优划分属性 $a_{*}$}}; \\
			9. \textbf{for} $a_{*}$ 的每一个值 $a_{*}^{'}$ do \\
			10. 为 node 生成一个分支; 令 $D_{*}^{'}$ 表示 $D$ 中在 $a_{*}$ 上取值为 $a_{*}^{'}$ 的样本子集; \\
			11. \textbf{if}: $D_{*}^{'}$ 为空 \textbf{then} \\
			12. 将分支结点标记为叶结点，\textbf{\textcolor{orange}{其类别标记为 $D$ 中样本最多的类}}; \textbf{return} \\
			13. \textbf{else} \\
			14. 以 TreeGenerate($D_{*}^{'}, A \setminus \{a_{*}^{'}\}$) 为分支结点 \\
			15. \textbf{end if} \\
			16. \textbf{end if} \\
			17. \textbf{end for} \\
			18. \textbf{输出}: 以 node 为根结点的一棵决策树 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\subsection{决策树的特征选择}
\subsubsection{\textbf{信息增益}（Information Gain）}
信息增益是ID3（Iterative Dichotomiser，迭代二分器 Three）算法中应用的特征选择指标。

\textbf{信息熵}（entropy）是度量样本集合“纯度”最常用的一种指标。假定当前样本集合D中第k类样本所占的比例为$p_{k}$，则D的信息熵定义为:
\setcounter{equation}{0} %新的一章，将编号公式重设为1
\begin{equation}Ent(D)=-\sum_{k=1}^{|y|}p_k\log_2p_k\end{equation}

在计算信息熵时，我们约定：若p=0，则$p\log_2p=0$。由上式可以看出Ent(D)的最小值为0，最大值为$\log_2|y|$。Ent(D)的值越小，D的纯度越高。信息增益直接以信息熵为基础，计算当前划分对信息熵所造成的变化。若离散属性a的取值是：$\{a^1,a^2,...,a^V\}$，第V个分支$D^{\nu}$表示D中在a上取值$D^{\nu}$等于的样本集合，那么以属性a对数据集D进行划分所获得的信息增益为：
\begin{equation}Gain(D,a)=Ent(D)-\sum_{\nu=1}^V\frac{|D^\nu|}{|D|}Ent(D^\nu)\end{equation}
其中，Ent(D)指的是划分前的信息熵，$\frac{\mid D^\nu\mid}{\mid D\mid}$指的是第$\nu$个分支的权重，$Ent(D^\nu)$指的是划分后的信息熵。
\subsubsection{\textbf{增益率}（Gain Ratio）}
增益率是C4.5决策树算法中应用的特征选择指标。
上一点中提到的信息增益有一个明显的弱点，那就是对可取值数目较多的属性有所偏好。比如以\textbf{索引编号}作为划分属性，就会导致每个分支结点都有且仅有一个样本，纯度达到最大，很明显毫无泛化能力。为了消除这一不利影响，著名的C4.5算法不直接使用信息增益，而是选择了增益率作为特征选择标准。

增益率的定义是：
\begin{equation}Gainratio(D,a)=\frac{Gain(D,a)}{IV(a)}\end{equation}
其中a的\textbf{固有值}（intrinsic value）定义为：
\begin{equation}IV(a)=-\sum_{\nu=1}^V\frac{|D^\nu|}{|D|}\mathrm{log}_2\frac{|D^\nu|}{|D|}\end{equation}
属性a的可能取值数目越多（即V越大），则IV(a)的值通常就越大。

因为增益率准则对可取值数目较少的属性有所偏好，所以C4.5算法没有直接选择增益率最大的候选属性，而是采用一个\textbf{启发式}：先从候选划分属性中找出信息增益高于平均水平的，再从中选取增益率最高的。
\subsubsection{\textbf{基尼指数}（Gini Index）}
基尼指数是CART（Classification and Regression Tree）算法中应用的特征选择指标。

CART是在给定输入随机变量条件下输出随机变量的条件概率分布的学习方法。CART算法通过选择最优特征和特征值进行划分，将输入空间也就是特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出条件概率分布。其主要包括回归树和分类树两种。回归树用于目标变量为连续型的建模任务，其特征选择准则用的是\textbf{平方误差最小准则}。分类树用于目标变量为离散型的的建模任务，其特征选择准则用的是基尼指数，这也有别于此前ID3的信息增益准则和C4.5的增益率准则。无论是回归树还是分类树，其算法核心都在于递归地选择最优特征构建决策树。

数据集D的纯度可以由\textbf{基尼值}来度量，基尼值定义为：
\begin{equation}Gini(D)=\sum_{k=1}^{|y|}\sum_{k^{\prime}\neq k}p_kp_{k^{\prime}}=1-\sum_{k=1}^{|y|}p_k^2\end{equation}
式(5)中，Gini(D)反映了从D中随机抽取两个样例，其类别标记不一致的概率。因此，Gini(D)越小，数据集D的纯度越高。属性a的基尼指数定义为：
\begin{equation}GiniIndex(D,a)=\sum_{\nu=1}^V\frac{|D^\nu|}{|D|}Gini(D^\nu)\end{equation}

在候选属性集合中，选取那个使划分后基尼指数最小的属性即可。
\subsection{决策树的剪枝处理}
研究表明，划分选择的各种准则虽然对决策树的尺寸有较大影响，但对泛化性能的影响很有限，例如信息增益与基尼指数产生的结果，仅在约2\%的情况下不同。而剪枝方法和程度对决策树泛化性能的影响更为显著，在数据带噪声时甚至可能将泛化性能提升25\%。为什么会产生这样的结果？这是因为剪枝（pruning） 是决策树对付“过拟合”的主要手段！剪枝处理是为了尽可能正确分类训练样本，通过主动去掉一些分支来降低过拟合的风险的操作。

剪枝处理的基本策略有两种：

情形一：\textbf{预剪枝}（pre-pruning），即\textbf{提前终止某些分支的生长}。因为其在训练开始前就提前砍掉了很多分支，所以在降低过拟合风险的同时也节省了时间及成本的开销。但是，预剪枝基于“贪心”本质禁止这些分支展开，这样也会给预剪枝决策树带来了欠拟含的风险。

情形二：\textbf{后剪枝}（post-pruning），即\textbf{先生成一个完整树，再回头剪枝}。一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往预剪枝决策树更好。但是由于要自底向上对树中的所有非叶结点进行逐一考察，其训练时间和成本开销会大得多。

剪枝过程中需评估剪枝前后决策树的优劣，即要使用到机器学习中的\textbf{评估方法}。如留出法、交叉验证法和自助法。
\subsection{缺失值处理策略}
如果遇到某些样本数据丢失的情况，我们如何解决以下两个问题？Q1：如何进行划分属性选择？Q2：给定划分属性，若样本在该属性上的值缺失，如何进行划分？

基本处理思路可以概括为“\textbf{样本赋权，权重划分}”，这个思路不仅适用于决策树模型，也可适用于集成学习中的提升树模型。  

针对Q1，用无缺样本代替总体来进行计算，从而进行划分。注意，这里“无缺样本”指的并不是“所有属性都完整的样本”，而是对于某一属性，集合内该属性没有缺失的样本（有可能一个样本可以参与属性a的计算但是不可以参与属性b的计算）给出新的信息增益计算公式：
\begin{equation}Gain(D,a)=\rho\times Gain(D,a)=\rho\times[Ent(D)-\sum_{\nu=1}^V\tilde{r}_\nu Ent(\tilde{D}^\nu)]\end{equation}
其中，$Ent(\tilde{D})=-\sum_{k=1}^{|y|}\widetilde{p}_k\log_2\widetilde{p}_k$。
对于属性a，$\rho $是无缺样本所占比例，$\tilde{p}_{k}$是无缺样本中第k类样本所占比例，$\tilde{r}_{\nu}$是无缺样本中在属性a上取值为$a^{\nu}$的所占比例。

针对Q2，以$\tilde{r}_{\nu}$为新权重，划分到每个子结点中（每个子结点的权重不同，和为1）。权重划分进入分支的过程可以理解为\textbf{把有值的样本进入各个属性划分结果（后验概率）当作了无值样本进入各个属性划分结果（先验概率）}。


决策树的原理相对来说很容易理解，它的提出是受到了香农提出的信息论的影响。所以，无论是ID3、CART还是C4.5，它们的划分准则指标都与信息相关。然而令人耳目一新的是，我们也可以从条件概率分布的角度理解决策树，在第二节中呈现出来了，这非常美妙！

最后是代码，可以去尝试使用经典的数据集“今天你是否要打高尔夫”，利用sklearn库中的tree模型构建决策树。笔者构建的ID3大致效果如图4，囿于篇幅，代码就不展示了。
\begin{figure}[ht] % 'h' 表示在当前位置插入图像
	\centering
	\includegraphics[width=0.9\textwidth]{"figures/图4.png"} % 调整图片宽度为页面宽度的 80%
	\caption{一棵ID3决策树} % 为图片添加标题
	\label{fig:example} % 为图片设置标签，用于引用
\end{figure}